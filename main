import numpy as np
import pandas as pd
import random
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFECV
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_predict
import os
import warnings
from pymoo.indicators.hv import HV
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

warnings.filterwarnings("ignore")

# Define the feature selection problem
class FeatureSelectionProblem:
    def __init__(self, X, y, clf):
        self.X = X
        self.y = y
        self.clf = clf

    def evaluate(self, individual):
        # Select the features
        selected_features = [index for index in range(len(individual)) if individual[index] == 1]
        if len(selected_features) == 0:
            return 0
        
        X_selected = self.X.iloc[:, selected_features]
        
        # Normalize the selected features
        scaler = MinMaxScaler()
        X_selected_normalized = scaler.fit_transform(X_selected)

        # Train and evaluate the classifier
        X_train, X_test, y_train, y_test = train_test_split(X_selected_normalized, self.y, test_size=0.3, random_state=42)
        
        # Perform cross-validation to prevent data leakage
        #y_pred = cross_val_predict(self.clf, X_selected, self.y, cv=5)
        self.clf.fit(X_train, y_train)
        y_pred = self.clf.predict(X_selected_normalized)

        # Obj_1: Error rate
        incorrect_predictions = sum(y_pred != self.y)  # Number of correct predictions
        error_rate = 1 - incorrect_predictions / len(self.y)  # Error rate
        
        # Obj_2: Feature selected ratio
        FS_rate = len(selected_features) / self.X.shape[1]  # Feature selected ratio

        #accuracy = accuracy_score(self.y, y_pred)
        # Calculate the number of selected features
        #num_features = np.sum(x)
        return round(error_rate,4), round(FS_rate, 4)

def read_file(files_name):
    # Read file and extract data file
    full_path = os.path.abspath(__file__) # Get the full path of the script     
    script_directory = os.path.dirname(full_path) # Get the directory of the script
    data_file = os.path.join(script_directory,files_name[0]) 
    columns_file = os.path.join(script_directory, files_name[1]) # wbcd.names , sonar.names

    # -------- Datase: (vehicle) ----------------------------------
    if files_name[0] == 'vehicle.dat': # If the file name is empty
        column_names = ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE CIRCULARITY', 'RADIUS RATIO', 'PR.AXIS ASPECT RATIO', 
                        'MAX.LENGTH ASPECT RATIO', 'SCATTER RATIO', 'ELONGATEDNESS', 'PR.AXIS RECTANGULARITY', 'MAX.LENGTH RECTANGULARITY', 
                        'SCALED VARIANCE ALONG MAJOR AXIS','SCALED VARIANCE ALONG MINOR AXIS', 'SCALED RADIUS OF GYRATION', 'SKEWNESS ABOUT MAJOR AXIS', 
                        'SKEWNESS ABOUT MINOR AXIS', 'KURTOSIS ABOUT MINOR AXIS', 'KURTOSIS ABOUT MAJOR AXIS', 'HOLLOWS RATIO', 'class_']
        
        # reading dataset from file
        with open(data_file,'r') as data_:
            data = data_.readlines()
            dataset = pd.DataFrame(columns=column_names)
            for line in data:
                x = line.split()
                dataset.loc[len(dataset)] = x
            
            # Binary target column: , 0: opel, 1: saab, 2: van, 3: bus
            dataset['class'] = [0 if row == 'opel' else 1 if row == 'saab' else 2 if row == 'van' else 3 for row in dataset['class_']]
            
            dataset = dataset.drop(columns=['class_'])
        return dataset
    
    # -------- Datase: (clean1.data) ----------------------------------
    if files_name[0] == 'clean1.data': # If the file name is empty
        # reading dataset from file
        columns_name = []
        # Reading Column names from file
        with open(columns_file,'r') as file_: 
            columns = file_.readlines()
            for idx, line in enumerate(columns): # extract values
                if idx == 0: 
                    columns_name.append('target_')   # add column for target column
                    continue
                x = line.split()
                columns_name.append(x[0])
                #column_data.append(x[1])

        # Reading dataset from file
        with open(data_file,'r') as data_: 
            data = data_.readlines()
            dataset = pd.DataFrame(columns=columns_name)
            for line in data:
                x = line.split(',')
                dataset.loc[len(dataset)] = x

        # Bnary target column: 0: MUSK, 1: NON-MUSK
        dataset['class'] = [0 if row[0:4] == 'MUSK' else 1 for row in dataset['target_'].str.strip()]

        # Apply EDAs 
        dataset = dataset.drop(columns=['molecule_name:', 'conformation_name:', 'f166:','target_'])  # Drop unrelivant columns
                    

    return dataset

def initialize_population(pop_size, num_features, seed_):
    random.seed(seed_)
    return [[random.randint(0, 1) for _ in range(num_features)] for _ in range(pop_size)]

def Wrapper_Fs(Feature, Target, seed_):
    random.seed(seed_)
    clf = LogisticRegression(random_state=seed_, max_iter=1000)
    cv = StratifiedKFold(5)

    # Normalize the features
    scaler = MinMaxScaler()
    Feature_normalized = scaler.fit_transform(Feature)

    rfecv = RFECV(
        estimator=clf,
        step=1,
        cv=cv,
        scoring="accuracy",
        #min_features_to_select=min_features_to_select,
        n_jobs=2,
    )
    rfecv.fit(Feature_normalized, Target)
    #predictions = rfecv.predict(Feature)  # Predictions of the best subset of features
    selected_features = Feature.columns[rfecv.support_]  # Names of selected features
    return selected_features

def get_rank(individual, fronts):
    for rank, front in enumerate(fronts):
        if individual in front:
            return rank
    return None

def get_crowding_distance(individual_idx, crowding_distances):
    """
    Get the crowding distance for a particular individual index.
    
    Parameters:
    individual_idx: The index of the individual whose crowding distance is to be found.
    crowding_distances: List of tuples where each tuple contains (index, crowding distance).
    
    Returns:
    The crowding distance of the individual, or None if the individual is not found.
    """
    for idx, distance in crowding_distances:
        if idx == individual_idx:
            return distance
    return None

def binary_tournament_selection(population, fronts, crowding_distances, k=2):
    winner = []
    for _ in range(k):
        # Select two individuals randomly
        #selected_idx = random.sample(population, k)
        selected_idx  = random.sample(range(len(population)), k)
        
        # Get the rank of the selected individuals
        parent1_rank = get_rank(selected_idx[0], fronts)
        parent2_rank = get_rank(selected_idx[1], fronts)
        # Compare ranks of the selected individuals
        if parent1_rank < parent2_rank:
            winner.append(selected_idx[0])
        elif parent1_rank > parent2_rank:
            winner.append(selected_idx[1])

        elif parent1_rank == parent2_rank: # if the ranks are the same for two selection individuals
            if crowding_distances[selected_idx[0]] > crowding_distances[selected_idx[1]]:
                winner.append(selected_idx[0])
            else:
                winner.append(selected_idx[1])
    
    return winner[0], winner[1]

def genetic_operation(population, fronts, crowding_distances, model, pop_size, num_generations, crossover_prob, mutation_prob):

    offspring = []
    while len(offspring) < pop_size:
        # Select two parents using tournament selection
        parent1, parent2 = binary_tournament_selection(population, fronts, crowding_distances, 2)
        parent1 = population[parent1]
        parent2 = population[parent2] 
        # Crossover
        if random.random() < crossover_prob:
            child1, child2 = crossover(parent1, parent2)
        else:
            child1, child2 = parent1, parent2
        # Mutation
        if random.random() < mutation_prob:
            child1 = mutate(child1)
        if random.random() < mutation_prob:
            child2 = mutate(child2)
        offspring.extend([child1, child2])

    # Evaluate the offspring
    offspring_fitnesses = [model.evaluate(ind) for ind in offspring]
    #for ind, fit in zip(offspring, fitnesses):
    #    ind.fitness = fit

    # Non-dominated sorting and crowding distance calculation for Offspring
    offspring_fronts = non_dominated_sort(offspring, offspring_fitnesses)
    #offspring_crowding_distances = calculate_crowding_distance(offspring_fronts, offspring_fitnesses)
    #population = population[:pop_size]

    return offspring, offspring_fitnesses, offspring_fronts

def crossover(parent1, parent2):
    crossover_point = random.randint(1, len(parent1) - 1)
    child1 = np.concatenate((parent1[:crossover_point], parent2[crossover_point:]))
    child2 = np.concatenate((parent2[:crossover_point], parent1[crossover_point:]))
    return child1, child2

def mutate(individual):
    mutation_point = random.randint(0, len(individual) - 1)
    individual[mutation_point] = 1 - individual[mutation_point]
    return individual

def calculate_crowding_distance(fronts, fitnesses):
    """
    Calculate the crowding distance for each individual in the fronts.
    
    Parameters:
    fronts: List of fronts where each front is a list of individual indices.
    fitnesses: List of fitness values where each index corresponds to the index of the individual in the population.
    
    Returns:
    Dictionary of crowding distances where the key is the individual's index.
    """
    num_solutions = len(fitnesses)
    num_objectives = len(fitnesses[0])

    # Initialize crowding distances
    crowding_distances = np.zeros(num_solutions)

    # Calculate crowding distances for each front
    for front in fronts:
        if len(front) == 0:
            continue
        
        for k in range(num_objectives):
            # Sort the front by the objective k values
            sorted_front = sorted(front, key=lambda x: fitnesses[x][k])
            min_value = fitnesses[sorted_front[0]][k]
            max_value = fitnesses[sorted_front[-1]][k]

            # Assign infinite distance to boundary solutions
            crowding_distances[sorted_front[0]] = float('inf')
            crowding_distances[sorted_front[-1]] = float('inf')

            # Calculate the crowding distance for each interior solution
            if max_value != min_value:  # Avoid division by zero
                for j in range(1, len(sorted_front) - 1):
                    crowding_distances[sorted_front[j]] += (
                        (fitnesses[sorted_front[j + 1]][k] - fitnesses[sorted_front[j - 1]][k]) /
                        (max_value - min_value)
                    )
    
    return crowding_distances

def non_dominated_sort(population, fitnesses):
        """
    Perform non-dominated sorting on the population.
    
    Parameters:
    population: List of individuals.
    fitnesses: List of fitness values where each index corresponds to the index of the individual in the population.
    
    Returns:
    List of fronts, where each front is a list of individuals.
    """
        fronts = []
        front = []
        
        domination_count = {i: 0 for i in range(len(population))}
        dominated_solutions = {i: [] for i in range(len(population))}
        
        for i, ind in enumerate(population):
            for j, other in enumerate(population):
                if i != j:
                    if dominates(fitnesses[i], fitnesses[j]):
                        dominated_solutions[i].append(j)
                    elif dominates(fitnesses[j], fitnesses[i]):
                        domination_count[i] += 1
            
            if domination_count[i] == 0:
                front.append(i)
        
        fronts.append(front)
        i = 0
        
        while len(fronts[i]) > 0:
            next_front = []
            for ind in fronts[i]:
                for other in dominated_solutions[ind]:
                    domination_count[other] -= 1
                    if domination_count[other] == 0:
                        next_front.append(other)
            i += 1
            fronts.append(next_front)
        
        # Remove the last empty front
        if not fronts[-1]:
            fronts.pop()
        
        # Store the sorted individuals in a separate variable
        sorted_individuals = [population[ind] for front in fronts for ind in front]
        
        return fronts

def dominates(fitness1, fitness2):
    """
    Check if fitness1 dominates fitness2 based on two objectives.
    
    Parameters:
    fitness1: List of fitness values for the first individual.
    fitness2: List of fitness values for the second individual.
    
    Returns:
    True if fitness1 dominates fitness2, False otherwise.
    """
    return all(x <= y for x, y in zip(fitness1, fitness2)) and any(x < y for x, y in zip(fitness1, fitness2))

def select_next_generation(new_population, new_population_fronts, new_population_crowding_distances, population_size):
    """
    Prepare the population for the next generation.
    
    Parameters:
    new_population: List of individuals in the new population.
    new_population_fronts: List of fronts where each front is a list of individual indices.
    new_population_crowding_distances: List of tuples where each tuple contains (index, crowding distance).
    population_size: Desired size of the next generation population.
    
    Returns:
    List of individuals selected for the next generation.
    """
    next_generation = []
    crowding_distance_dict = {idx: dist for idx, dist in enumerate(new_population_crowding_distances)}
    
    for front in new_population_fronts:
        if len(next_generation) + len(front) > population_size:
            if len(next_generation) == population_size: # If the population is already full
                break
            # Sort the front by crowding distance in descending order
            front.sort(key=lambda idx: crowding_distance_dict[idx], reverse=True)
            [next_generation.append(new_population[ind]) for ind in front[:population_size - len(next_generation)]]
            break
        else:
            # Add all individual of front to new_population
            [next_generation.append(new_population[ind]) for ind in front]
    
    return next_generation

def hyper_volume_clac(pareto_individuals, pareto_individual_fitness):
    """
    Calculate the hyper-volume of the Pareto front.
    
    Parameters:
    pareto_individuals: List of individuals in the Pareto front.
    pareto_individual_fitness: List of fitness values for the Pareto front individuals.
    
    Returns:
    float: The hyper-volume of the Pareto front.
    """
    # Convert fitness values to a numpy array
    #pareto_individual_fitness = np.array(pareto_individual_fitness)
    
    # Normalize the fitness values
    scaler = MinMaxScaler()
    pareto_individual_fitness_normalized = scaler.fit_transform(pareto_individual_fitness)

    # Define the reference point (this should be chosen based on the problem context)
    reference_point = [1.1] * len(pareto_individual_fitness[0])
    #reference_point = np.max(pareto_individual_fitness_normalized, axis=0)
    
    # Compute the hyper-volume
    hv = HV(ref_point=reference_point)
    hyper_volume = hv.do(pareto_individual_fitness_normalized)
    
    return hyper_volume

def plot_pareto_front(pareto_individual_fitness, dataset_name, run_no):
    """
    Plot the Pareto front fitness values.
    
    Parameters:
    pareto_individuals: List of individuals in the Pareto front.
    pareto_individual_fitness: List of fitness values for the Pareto front individuals.
    dataset_name: Name of the dataset.
    run_no: Run number.
    """
    pareto_individual_fitness = np.array(pareto_individual_fitness)
    
    # Create a scatter plot
    plt.scatter(pareto_individual_fitness[:, 0], pareto_individual_fitness[:, 1], color='b', marker='o')
    plt.title(f'Pareto Front Fitness Values for {dataset_name} (Run {run_no})')
    plt.xlabel('Objective 1: Error Rate')
    plt.ylabel('Objective 2: Feature Selected Ratio')
    
    plt.grid(True)
    plt.show()
    #plt.close()
    # Save the plot as an image file
    #plt.savefig(f'pareto_front_{dataset_name}_run_{run_no}.png')
    

def print_summary(hv, mean_, std_):
    """
        Creates a pandas DataFrame table with the given data.
    """
    # First column
    first_column = ['Run 1','Run 2','Run 3']

    # Create a dictionary with the two lists as values
    data = {'': first_column, 'Hyper-volume': hv}

    # Create a pandas DataFrame from the dictionary
    data_table = pd.DataFrame(data)

    # Create a new DataFrame with the mean and concatenate it with (data_table)
    mean_row = pd.DataFrame({'': ['Mean'], 'Hyper-volume': [mean_]})
    data_table = pd.concat([data_table, mean_row], ignore_index=True)

    # Create a new DataFrame with the stander deviation and concatenate it with (data_table)
    std_row = pd.DataFrame({'': ['STD'], 'Hyper-volume': [std_] })
    data_table = pd.concat([data_table, std_row], ignore_index=True)

    return data_table

def main():
    # Setttings for the algorithm
    population_size = 50
    num_generations = 5
    crossover_prob = 0.5
    mutation_prob = 0.1
    runs = 3
    seed_ = [20, 30, 40, 50, 60]

    file_name = ['clean1.data','vehicle.dat']
    columns_file = ['clean1.names']

    for i, datafile_ in enumerate(file_name):
        hyper_volumes = []

        # Read dataset and columns from file 
        dataset = read_file([datafile_, columns_file[0]])
        Feature = dataset.iloc[:, :-1]  # Features
        
        """
        Binary target column:
                for (vehicle) dataset 0: opel, 1: saab, 2: van, 3: bus
                for (clean1) dataset 0: MUSK, 1: NON-MUSK 
        """
        Target = dataset.iloc[:, -1]

        for run in range(runs):
            print(f' Run {run+1} of {runs} in progress, dataset {datafile_}')
            # Feature selection, then data transformation
            feature_selected = Wrapper_Fs(Feature, Target, seed_[run])
            datset_fs = Feature[feature_selected]

            # initialize popoulation
            fs_population = initialize_population(population_size, datset_fs.shape[1], seed_[run])

            for gen in range(num_generations):
                print('Generation: ', gen)
                # Initialize the problem
                model = FeatureSelectionProblem(datset_fs, Target, RandomForestClassifier(n_estimators=50))
                
                # FITNESS FUNCTION: Evaluate the fitness of each individual in the population
                fitnesses = [model.evaluate(ind) for ind in fs_population]
                #for ind, fit in zip(fs_population, fitnesses):
                #    ind.fitness = fit
                
                # NON-DOMINANCE SORTING: Calculate the Non-dominance fronts
                fronts = non_dominated_sort(fs_population, fitnesses)
                
                # CROWDING DISTANCE: Calculate the crowding distance of each individual in each front
                crowding_distances = calculate_crowding_distance(fronts, fitnesses)

                # ---------GENETIC OPERATION: ------------------------
                # Perform the genetic operation
                offspring, offspring_fitnesses, offspring_fronts = genetic_operation(fs_population, fronts, crowding_distances, model, 
                                                                                     population_size, num_generations, crossover_prob, mutation_prob)
                # -------- COMBINE POPULATIONS -------------------------
                # combine the population and offspring into a single population
                comb_pop = fs_population + offspring
                comb_pop_fitnesses = [model.evaluate(ind) for ind in comb_pop]

                # Combine the fronts of the population and offspring
                comb_pop_fronts = non_dominated_sort(comb_pop, comb_pop_fitnesses)
                
                # Calculate the crowding distance of the new population
                comb_pop_crowding_distances = calculate_crowding_distance(comb_pop_fronts, comb_pop_fitnesses)
                
                # Select the next generation
                next_generation_populations = select_next_generation(comb_pop, comb_pop_fronts, comb_pop_crowding_distances, population_size)
                fs_population = next_generation_populations                
            
            # ------------ CALCULATE THE FINAL FRONT -------------------
            # Calculate non-dominante sorting, fitnes fucntion, crowding distance for the final population for current run
            final_fitnesses = [model.evaluate(ind) for ind in fs_population]
            final_fronts = non_dominated_sort(fs_population, final_fitnesses)
            final_crowding_distances = calculate_crowding_distance(final_fronts, final_fitnesses)
            
            # Select pareto individual from the final front
            pareto_individuals = [fs_population[idx] for idx in final_fronts[0]]
            pareto_individual_fitness = [final_fitnesses[fitness] for fitness in final_fronts[0]]
            print('Pareto individuals number: ', final_fronts[0])
            print('Pareto individuals fitness: ', pareto_individual_fitness)

            # ----------------- CLASSIFY THE ENTIRE DATASET FEATURES -------------------
            # dataset for entire features
            all_features_model = FeatureSelectionProblem(Feature, Target, RandomForestClassifier(n_estimators=50))
            all_features_population = [[1 for _ in range(Feature.shape[1])] for _ in range(1)]
            all_features_fitnesses = [all_features_model.evaluate(ind) for ind in all_features_population]
            print('Error rate using the entire dataset features: ', all_features_fitnesses[0][0])

            # ---------------- HYPER-VOLUME --------------------
            # Calculate the hyper-volume of the final front
            hv = hyper_volume_clac(pareto_individuals, pareto_individual_fitness)
            hyper_volumes.append(hv)  # Store the hyper-volume for each run
            print('Hyper-volume: ', round(hv, 4))

            # ----------------- PLOT THE RESULTS -------------------
            # Plot the Pareto front
            plot_pareto_front(pareto_individual_fitness, datafile_, run)
            print('------------------------------------------------')

        # ---------------- SUMMARY RESULTS ----------------------------------
        # Calculate the average hyper-volume across all runs
        avg_hv = np.mean(hyper_volumes)
        std_hv = np.std(hyper_volumes)
        
        # Print the summary of hyper-volumes
        summary = print_summary(hyper_volumes, round(avg_hv, 4), round(std_hv, 4))
        print(summary)

        # ------------------ CLASSIFIED ENTIRE DATASET FEATURES -----------------------
        
        
if __name__ == '__main__':
    main()